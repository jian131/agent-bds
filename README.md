# ğŸ  BDS Agent - Há»‡ thá»‘ng tÃ¬m kiáº¿m & quáº£n lÃ½ tin BÄS tá»± Ä‘á»™ng

Há»‡ thá»‘ng AI Agent tá»± Ä‘á»™ng thu tháº­p, lÆ°u trá»¯ vÃ  tÃ¬m kiáº¿m thÃ´ng tin báº¥t Ä‘á»™ng sáº£n tá»« nhiá»u nguá»“n.

## âœ¨ TÃ­nh nÄƒng chÃ­nh

- **ğŸ¤– AI Agent thÃ´ng minh**: Tá»± Ä‘á»™ng tÃ¬m kiáº¿m vÃ  thu tháº­p dá»¯ liá»‡u tá»« nhiá»u nguá»“n
- **ğŸŒ Multi-source scraping**: Chá»£ Tá»‘t, Batdongsan.com.vn, Mogi, Alonhadat, Facebook, Google
- **âœ… Data validation**: Kiá»ƒm tra sá»‘ Ä‘iá»‡n thoáº¡i, giÃ¡ há»£p lÃ½, Ä‘á»‹a chá»‰ thá»±c
- **ğŸ” Semantic search**: TÃ¬m kiáº¿m ngá»¯ nghÄ©a vá»›i ChromaDB
- **ğŸ“Š Database + Backup**: PostgreSQL + Google Sheets
- **ğŸ”” Notifications**: Telegram Bot alerts
- **ğŸ¯ 100% FREE stack**: Ollama local LLM, browser-use automation

## ğŸ› ï¸ Tech Stack

| Component          | Technology             |
| ------------------ | ---------------------- |
| LLM                | Ollama (qwen2.5:14b)   |
| Browser Automation | browser-use            |
| Backend            | FastAPI                |
| Database           | PostgreSQL             |
| Vector DB          | ChromaDB               |
| Frontend           | Next.js 14 + Shadcn/UI |
| Scheduler          | APScheduler            |
| Backup             | Google Sheets API      |
| Notifications      | Telegram Bot API       |

## ğŸ“ Project Structure

```
bds-agent/
â”œâ”€â”€ main.py                 # Entry point
â”œâ”€â”€ config.py               # Settings (Pydantic)
â”œâ”€â”€ docker-compose.yml      # PostgreSQL + Redis
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ search_agent.py     # Core AI agent
â”‚   â”œâ”€â”€ tools.py            # Custom tools
â”‚   â””â”€â”€ prompts.py          # LLM prompts
â”‚
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ database.py         # SQLAlchemy models
â”‚   â”œâ”€â”€ vector_db.py        # ChromaDB wrapper
â”‚   â””â”€â”€ sheets.py           # Google Sheets
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ scraper.py          # Scraper orchestrator
â”‚   â”œâ”€â”€ validator.py        # Data validation
â”‚   â””â”€â”€ matcher.py          # Buyer-seller matching
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ routes/             # FastAPI endpoints
â”‚
â”œâ”€â”€ frontend/               # Next.js app
â”‚
â””â”€â”€ scheduler/
    â””â”€â”€ jobs.py             # Background jobs
```

## ğŸš€ Quick Start

### 1. Prerequisites

- Python 3.11+
- Docker & Docker Compose
- Ollama installed locally
- Node.js 18+ (for frontend)

### 2. Install Ollama & Model

```bash
# Install Ollama (Windows)
# Download from https://ollama.ai/download

# Pull the model
ollama pull qwen2.5:14b

# Verify
ollama list
```

### 3. Setup Project

```bash
# Clone repo
cd bds-agent

# Create virtual environment
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Install browser-use playwright browsers
python -m playwright install chromium
```

### 4. Configure Environment

```bash
# Copy example env
copy .env.example .env  # Windows
# cp .env.example .env  # Linux/Mac

# Edit .env with your settings
```

### 5. Start Database

```bash
# Start PostgreSQL & Redis
docker-compose up -d

# Verify
docker-compose ps
```

### 6. Run Agent

```bash
# Demo mode
python main.py demo

# Interactive mode
python main.py interactive

# Quick search
python main.py search "chung cÆ° 2PN Cáº§u Giáº¥y 2-3 tá»·"

# Start API server
python main.py api
```

## ğŸ“– Usage Examples

### Python API

```python
import asyncio
from agents.search_agent import RealEstateSearchAgent

async def main():
    agent = RealEstateSearchAgent()

    result = await agent.search(
        "TÃ¬m chung cÆ° 2PN Cáº§u Giáº¥y 2-3 tá»·",
        max_results=10,
        platforms=["chotot", "batdongsan"]
    )

    print(f"Found {result.total_found} listings")

    for listing in result.listings:
        print(f"- {listing['title']}")
        print(f"  Price: {listing['price_text']}")
        print(f"  URL: {listing['source_url']}")

    await agent.close()

asyncio.run(main())
```

### REST API

```bash
# Search
curl -X POST http://localhost:8000/api/search \
  -H "Content-Type: application/json" \
  -d '{"query": "chung cÆ° 2PN Cáº§u Giáº¥y 2-3 tá»·"}'

# Get listings
curl http://localhost:8000/api/listings

# Get listing detail
curl http://localhost:8000/api/listings/{id}
```

## ğŸ”§ Configuration

### Environment Variables

| Variable           | Description                | Default                    |
| ------------------ | -------------------------- | -------------------------- |
| `OLLAMA_MODEL`     | Ollama model name          | `qwen2.5:14b`              |
| `OLLAMA_BASE_URL`  | Ollama server URL          | `http://localhost:11434`   |
| `DATABASE_URL`     | PostgreSQL connection      | `postgresql+asyncpg://...` |
| `HEADLESS_MODE`    | Run browser headless       | `false`                    |
| `SCRAPE_DELAY_MIN` | Min delay between requests | `2`                        |
| `SCRAPE_DELAY_MAX` | Max delay between requests | `5`                        |

### Price Validation by District

GiÃ¡ Ä‘Æ°á»£c validate theo khoáº£ng há»£p lÃ½ cho tá»«ng quáº­n (triá»‡u VND/mÂ²):

| Quáº­n      | Min | Max |
| --------- | --- | --- |
| HoÃ n Kiáº¿m | 100 | 300 |
| Ba ÄÃ¬nh   | 80  | 250 |
| TÃ¢y Há»“    | 80  | 250 |
| Cáº§u Giáº¥y  | 60  | 180 |
| HÃ  ÄÃ´ng   | 35  | 100 |
| ...       | ... | ... |

## ğŸ”’ Data Validation

Má»—i listing Ä‘Æ°á»£c validate:

1. **Required fields**: `source_url`, `title`
2. **Phone validation**: Format VN (0xxx-xxx-xxxx)
3. **Price validation**: Trong khoáº£ng há»£p lÃ½ cho khu vá»±c
4. **Deduplication**: Hash(url + phone + title)
5. **Spam detection**: Lá»c tin mÃ´i giá»›i, kÃ½ gá»­i

## ğŸ“Š Listing Schema

```json
{
  "id": "md5_hash",
  "title": "BÃ¡n chung cÆ° 2PN táº¡i Cáº§u Giáº¥y",
  "price_text": "3 tá»· 500 triá»‡u",
  "price_number": 3500000000,
  "area_m2": 85.5,
  "location": {
    "address": "123 ÄÆ°á»ng ABC",
    "ward": "NghÄ©a ÄÃ´",
    "district": "Cáº§u Giáº¥y",
    "city": "HÃ  Ná»™i"
  },
  "contact": {
    "name": "Anh Minh",
    "phone": "0912 345 678",
    "phone_clean": "0912345678"
  },
  "images": ["url1", "url2"],
  "source_url": "https://...",
  "source_platform": "chotot",
  "scraped_at": "2024-01-20T10:30:00Z",
  "property_type": "chung cÆ°",
  "bedrooms": 2,
  "bathrooms": 2
}
```

## ğŸ§ª Testing

```bash
# Run tests
pytest tests/ -v

# With coverage
pytest tests/ --cov=. --cov-report=html
```

## ğŸ³ Docker Deployment

```bash
# Build & run all services
docker-compose -f docker-compose.yml up -d

# View logs
docker-compose logs -f

# Stop
docker-compose down
```

## ğŸ“ Development

### Code Style

```bash
# Format
black .

# Lint
ruff check .

# Type check
mypy .
```

### Adding New Platform

1. Add platform config to `config.py`:

```python
SCRAPING_PLATFORMS["newplatform"] = {
    "name": "New Platform",
    "base_url": "https://...",
    "priority": 7,
}
```

2. Implement scraper in `agents/search_agent.py`:

```python
async def _search_newplatform(self, intent: SearchIntent) -> list[dict]:
    # Implementation
    pass
```

## âš ï¸ Legal Notice

- This tool is for educational purposes only
- Respect robots.txt and terms of service
- Use reasonable delays between requests
- Do not overload target websites

## ğŸ“„ License

MIT License

## ğŸ¤ Contributing

1. Fork the repo
2. Create feature branch
3. Commit changes
4. Open PR

---

**Built with â¤ï¸ using browser-use + Ollama**
